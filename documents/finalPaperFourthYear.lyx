#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass IEEEtran-CompSoc
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style agsm
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
On the use of Deep Learning for Open World Person Re-Identification in Thermal
 Imagery
\end_layout

\begin_layout Author
T.A.
 Robson, supervised by T.P.
 Breckon
\end_layout

\begin_layout Section*
Abstract
\end_layout

\begin_layout Standard
Although use of thermal imagery currently poses significant advantages for
 24/7 surveillance in terms of the visibility of targets under all environmental
 conditions, a key limitation is the lack of colour information.
 Person re-identification across multiple cameras is a key research problem
 within the domain of visual surveillance and a important challenge for
 the future deployment of thermal sensing as an autonomous sensor technology.
 Many current approaches to the problem rely on colour features 
\begin_inset CommandInset citation
LatexCommand cite
key "Gong2014"

\end_inset

.
 We have previously attempted to use a set of similar features to solve
 the thermal re-identification problem with little success 
\begin_inset CommandInset citation
LatexCommand cite
key "Robson2017"

\end_inset

, and are now exploring alternatives.
 
\end_layout

\begin_layout Standard
The aim of this project is to build a system that can detect a person within
 thermal imagery in real time, distinguish a person from other objects and
 track a person moving through a scene in real time.
 This is done using a combination of a Mixture of Gaussians (MoG) background
 subtractor, a Histogram of Oriented Gradients (HOG) person detector and
 a Track-Learn-Detect (TLD) tracker.
 We must then be able to re-identify people based on those our system has
 already seen, which is done using a siamese Convolutional Neural Network
 (CNN) to determine if a pair of images contains the same person or two
 different people..
 It will be trained on a set dataset, and then tested on a set of different
 people.
 This means that we are aiming to solve the open world re-identification
 problem, rather than simply to train and test in a closed world on the
 same set of people, as many previous approaches have 
\begin_inset CommandInset citation
LatexCommand cite
key "Zheng2016"

\end_inset

.
 This enables us to deploy our solution on any camera system with any human
 targets.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add results/conclusions content
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard

\series bold
Keywords: 
\series default
Deep Learning, Siamese Network, Open World, Computer Vision, Person Re-Identific
ation, Re-ID, Person Tracking, Track-Learn-Detect, Thermal Imagery, Thermal
 Video
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction 
\begin_inset Note Note
status open

\begin_layout Plain Layout
ENSURE ALL FIGURE HEADINGS ARE NOT IN TITLE CASE
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A fundamental task for a distributed multi-camera surveillance system is
 to associate people across different camera views at different locations
 and times 
\begin_inset CommandInset citation
LatexCommand cite
key "Gong2014"

\end_inset

.
 This is referred to as the person re-identification problem 
\begin_inset CommandInset citation
LatexCommand cite
key "Gong2014"

\end_inset

 and is an interesting and important problem within the field of computer
 vision.
 From the previous work, we can see that a substantial amount of research
 that has been done on person re-identification, mainly revolving around
 the use of features or attributes of a person 
\begin_inset CommandInset citation
LatexCommand cite
key "Layne"

\end_inset

.
 However, much of this relies on the visible spectrum, with attributes of
 the form “red shirt” 
\begin_inset CommandInset citation
LatexCommand cite
key "Layne"

\end_inset

.
 However, in the modern world, thermal imagery is often used for 24/7 surveillan
ce when varying environmental conditions are present.
 Therefore, it is important that an effective re-identification system is
 developed to utilise this area of surveillance, as this problem has yet
 to be effectively solved.
\end_layout

\begin_layout Standard
There are many potential applications for this technology but the most important
 in the modern world would be to support human intelligence organisations.
 The surveillance data that a system like this can provide would be critical
 for crime-prevention, forensic analysis, and counter-terrorism activities
 in both civilian and governmental agencies alike.
 While this surveillance data is currently widely used by human operators,
 these operators have to be trained, which offsets the utility of this approach
 with training and staffing costs.
 The implementation of an automated re-identification system is therefore
 of great interest, as it would be very useful in supporting these human
 operators and enabling them to achieve better results more efficiently.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-Thermal"

\end_inset

 shows five different views of the same person that our features must be
 able to re-identify as being the same person.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement b
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/alpha 38.jpg
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/alpha 147.jpg
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/beta 86.jpg
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/gamma 26.jpg
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/gamma 1825.jpg
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-Thermal"

\end_inset

Several images showing the same person that our system must be able to re-identi
fy as the same person
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Whilst thermal imagery has many advantages, it is not able to identify colour,
 making features that rely on colour useless.
 Therefore, alternative features are required to facilitate re-identification.
 In our previous work in 
\begin_inset CommandInset citation
LatexCommand cite
key "Robson2017"

\end_inset

, we attempted to use features that a human would think were distinctive
 enough to facilitate re-identification.
 This had only limited success.
 Since our work in 
\begin_inset CommandInset citation
LatexCommand cite
key "Robson2017"

\end_inset

, the attention of the researchers of re-identification has shifted to deep
 learning based solutions, as shall be discussed in more detail in section
 2.
 These deep learning approaches can be split into two categories: closed
 world, where the system is trained and tested on the same set of people
 from a pre defined dataset, and open world, where the system is trained
 to learn what makes people different, so it can be applied to any dataset
 of people.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:openVsClosed"

\end_inset

 shows the difference between these two possible approaches to deep learning
 enabled re-identification.
\end_layout

\begin_layout Standard
In order to improve the performance and runtime of our new approach, we
 are replacing the Kalman filter used in 
\begin_inset CommandInset citation
LatexCommand cite
key "Robson2017"

\end_inset

 with an implementation of the Track-Learn-Detect (TLD) tracker, originally
 proposed by the authors of 
\begin_inset CommandInset citation
LatexCommand cite
key "Kalal2010"

\end_inset

.
 The aim here is to ensure that we do not have to pass every frame to the
 neural network, but instead, when a person has been identified, they can
 then be tracked for as long as they remain unobscured in the frame, and
 continue to be labelled as the same person.
\end_layout

\begin_layout Subsection
Aims of the Project
\end_layout

\begin_layout Standard
The intent of this project is to develop a system that can utilise an open
 world deep neural network, paired with a real time person detection system
 and TLD tracker, to solve the thermal re-identification problem, and improve
 on the performance of state of the art solutions, including our own previous
 work 
\begin_inset CommandInset citation
LatexCommand cite
key "Robson2017"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Achievements of the Project
\end_layout

\begin_layout Section
Related Work
\end_layout

\begin_layout Standard
We will now examine the previous work that has been done relating to the
 person re-identification problem.
\end_layout

\begin_layout Subsection
Feature-Based Attempts at Re-Identification
\end_layout

\begin_layout Standard
Re-identification in colour is a well researched area, particularly using
 distinctive features for this purpose.
 Many of these methods are discussed in 
\begin_inset CommandInset citation
LatexCommand cite
key "Gong2014"

\end_inset

, and have been widely researched and understood, so research has moved
 on to explore more complex methods.
 An important part of the current state of the art in this area is camera
 network layout and topology, as explored in 
\begin_inset CommandInset citation
LatexCommand cite
key "Martinel2016"

\end_inset

.
 Here, the technique of distance vector routing is employed to get an idea
 of the relative locations of the cameras, enabling the system to prioritise
 the people seen most recently by the closest camera, as these are most
 likely to be correct.
 This is done by first analysing the overlap between cameras, and then computing
 distance vectors and probabilities of going from one camera to another,
 reducing the time complexity of the re-identification process in the majority
 of cases.
 
\end_layout

\begin_layout Standard
The work in 
\begin_inset CommandInset citation
LatexCommand cite
key "Chu2014"

\end_inset

 is on a similar theme to 
\begin_inset CommandInset citation
LatexCommand cite
key "Martinel2016"

\end_inset

, but assumes a non-overlapping camera system.
 Each camera has entry and exit zones from its field of view, and if a person
 can get from one camera field of view to another they are directly connected.
 The system can then create what is referred to as a camera link model,
 using a temporal, spatial and appearance relation between the entry and
 exit zones of the cameras.
 These paths are obtained from training data, but the system itself learns
 how to recognise people by attributes, and uses the training data to estimate
 where they are most likely to have gone after leaving a given camera field
 of view.
 
\end_layout

\begin_layout Standard
The authors of 
\begin_inset CommandInset citation
LatexCommand cite
key "YiminWang2014"

\end_inset

 propose a different method for feature based identification, using a feature
 projection matrix to project image features of one camera to the feature
 space of another, to effectively eliminate the difference of feature distributi
ons between the two cameras.
 This feature projection matrix is obtained through supervised learning.
 The proposed method aims to use a simple gradient descent algorithm to
 accelerate and optimise the re-identification process by compensating for
 the inconsistency of feature distributions captured by different cameras.
 
\end_layout

\begin_layout Standard
The work in 
\begin_inset CommandInset citation
LatexCommand cite
key "Wu2012"

\end_inset

 emphasises the importance of making good use of all images and video frames
 captured of a target.
 The system proposed here creates a gallery of images of known individuals,
 with more images increasing the accuracy of the system.
 When a gallery exists for a target, this is referred to as multi-shot re-identi
fication, and single-shot re-identification when only one image is available
 in both the query and the gallery.
 For multi-shot re-identification, the authors propose to use geometric
 distance in another way by collaboratively approximating the query sets
 using all galleries, a method known as Collaborative Sparse Approximation.
 
\end_layout

\begin_layout Standard
Another approach, taken by the authors of 
\begin_inset CommandInset citation
LatexCommand cite
key "Liu2014"

\end_inset

, relies not only on extracting a set of features to use to compare people,
 but also on determining which of these features is the most discriminative
 for each person individually on the fly.
 This is achieved through the use of a random forest classifier, and functions
 well in an open world setting.
 However, since the time of publication of this work, the state of the art
 of this area has moved on to deep learning.
 
\end_layout

\begin_layout Standard
Our own previous attempt at thermal re-identification in 
\begin_inset CommandInset citation
LatexCommand cite
key "Robson2017"

\end_inset

 attempted to use features that a human would consider useful to re-identify
 people, such as an approximation of the shape of the target, an analysis
 of their gait and a measure of where the thermal hotspots of each person
 were.
 This performed resonably well for some features, but its inaccuracy in
 many situations led us to explore an alternative approach, using deep learning,
 based on the current state of the art in the area.
 
\end_layout

\begin_layout Subsection
Use of Deep Learning for Re-Identification
\end_layout

\begin_layout Standard
The use of deep learning to facilitate re-identification has been the driving
 force in the research community in this area in recent times 
\begin_inset CommandInset citation
LatexCommand cite
key "Matsukawa2017,Zheng2016,Zhang2014,Wang2016a,Almazan2018,Chung2017,Qian2017,Ahmed2015"

\end_inset

.
 As our ability to train deeper and deeper networks on larger and larger
 datasets has increased, largely thanks to modern improvements in graphics
 hardware, these approaches become more and more relevant and powerful.
 As before, much of this research is focused on the colour spectrum.
 The work of 
\begin_inset CommandInset citation
LatexCommand cite
key "Matsukawa2017"

\end_inset

 shows us how a combination of deep learning and human recognisable features
 can be used for re-identification.
 This works by training a neural network to recognise certain features that
 the authors consider to be discriminative.
 The result from the network is an ordered vector for the presence or absence
 of these features.
 However, as our previous work showed that we were unable to choose suitably
 descriminitive features for thermal re-identification, we will not be able
 to follow this approach.
 
\end_layout

\begin_layout Standard
Much of the rest of the previous deep learning work is split into open world
 and closed world.
 The work of 
\begin_inset CommandInset citation
LatexCommand cite
key "Zheng2016"

\end_inset

 presents a useful comparison between these two problems, as shown in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:openVsClosed"

\end_inset

.
 They refer to the closed world problem as Identification and the open world
 problem as Verification, with the justification that in closed world, the
 aim is to identify which of a given set of people the target is, where
 as in open world, the aim is to verify whether two targets are the same
 or not.
 These two approaches differ greatly in terms of input method, feature extractio
n and the loss function used.
 The authors of 
\begin_inset CommandInset citation
LatexCommand cite
key "Zheng2016"

\end_inset

 conclude, as we have, that the open world problem is more relevant to real
 world applications.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/open vs closed.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The difference between closed world (a) and open world (b) re-identification
 systems, from 
\begin_inset CommandInset citation
LatexCommand cite
key "Zheng2016"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:openVsClosed"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
A solution to the open world problem is attempted in 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang2014"

\end_inset

.
 The authors aim to combine feature learning and re-identification into
 one framework, which is a deep siamese CNN with an SVM placed on top of
 the network, after the last layer, to attempt open world re-identification.
 The results of this are promising for the potential use of such an architecture
, but fall short of the contemporary state of the art.
 The apparent reason for this is overfitting to the dataset used to train
 the architecture, and the authors hypothesise that if they could use a
 dataset with wider variation of people, improved results could be achieved.
 As this work is several years old, we can hypothesise that with modern
 graphics hardware, this model could realistically be trained on a much
 larger and more varied dataset, which should improve the results.
 This hypothesis is backed up by the authors of 
\begin_inset CommandInset citation
LatexCommand cite
key "Ahmed2015"

\end_inset

, who have implemented architecture and trained on a larger dataset which
 consistently outperforms the contemporary state of the art.
 
\end_layout

\begin_layout Standard
The work in 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang2016a"

\end_inset

 proposes a similar deep siamese CNN architecture, but also retains the
 single image representation.
 The purpose of this is to reduce the amount of computational work that
 must be done on both images together, as well to joint optimise the single
 image representation and cross image representation for improved accuracy
 at a lower computational cost.
 The authors claim to outperform the majority of contemporary state of the
 art methods.
 The work presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "Chung2017"

\end_inset

 uses a pair of siamese CNNs, one to learn spatial information and the other
 to learn temporal information.
 These features are then weighted, as the authors claim that spatial features
 are more discriminative than temporal features.
 This method outperforms or shows comparable results to the existing best
 performing methods.
 The work in 
\begin_inset CommandInset citation
LatexCommand cite
key "Qian2017"

\end_inset

 is another siamese CNN with the particular aim of making it work for people
 represented at different scales.
 This would be an interesting addition to our work, but is currently out
 of scope for us.
 
\end_layout

\begin_layout Standard
The authors of 
\begin_inset CommandInset citation
LatexCommand cite
key "Almazan2018"

\end_inset

 do not propose a novel architecture as such, but instead propose a set
 of good practices that should be followed for effective re-identification,
 and implement these in their siamese CNN.
 The recommended good practices that we intend to follow include using data
 augmentation, as this enables us to grow our dataset substantially whilst
 also introducing difficult examples that try to ensure that the network
 is not overfitting to a specific image format.
 We also will ensure that our network has a state of the art base architecture,
 taking inspiration from the architectures used in 
\begin_inset CommandInset citation
LatexCommand cite
key "Zheng2016,Zhang2014,Almazan2018,Wang2016a,Chung2017"

\end_inset

 to help us to design the most effective re-identification network possible.
 We also endeavour to follow the advice of using sufficiently large image
 resolution, but using a size large enough so that the network has plenty
 of information to learn from, but small enough so that each person is shown
 with sufficient detail and clarity.
 
\end_layout

\begin_layout Standard
These recommended good practices are pre-training for identity classification,
 sufficiently large image resolution, state of the-art base architecture
 and dataset augmentation with difficult examples.
 We will endeavour to follow these guidelines in our work in this project
 wherever possible.
 
\end_layout

\begin_layout Standard
The work of 
\begin_inset CommandInset citation
LatexCommand cite
key "Almazan2018"

\end_inset

 also compares several state of the art approaches on multiple datasets.
 The results of these are quoted in terms of precision, and the best are
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
NUMBERS
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
New paper (Consistent-Aware Deep Learning for Person Re-identification in
 a Camera Network) does try over full camera network, write paragraph about
 this, and others not doing it
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
TLD Tracker
\end_layout

\begin_layout Standard
The Track-Learn-Detect (TLD) tracker was first proposed by Zdenek Kalal
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Kalal2010"

\end_inset

.
 The idea of this tracker is to break down the person-tracking task into
 tracking, learning and detection.
 The tracker follows the object between frames.
 The detector corrects the tracker if necessary based on previous observations.
 The learning estimates the errors made by the detector and updates it to
 avoid these errors in the future.
 However, the main purpose of TLD is to be able to consistently follow the
 same object through as changing background, for example a car driving along
 a road being recorded from a helicopter.
 The authors have said that it performs sub optimally on varying targets
 such as people.
 During our early experimentation, we found that is was sufficiently capable
 of tracking a person through a single camera frame, but could not re-identify
 people across multiple cameras, or even returning from behind an obstruction.
\end_layout

\begin_layout Subsection
Relation to this Project
\end_layout

\begin_layout Standard
In this project we use elements from many of the papers
\begin_inset Note Note
status open

\begin_layout Plain Layout
which ones?
\end_layout

\end_inset

 discussed in this section.
 The TLD tracker will be used simply for tracking a person for as long as
 they remain visible in one camera view.
 Whether this person has been previously observed by the system or not will
 be determined by an open world siamese CNN, with an architecture similar
 to the related work.
 We take inspiration from 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang2016a"

\end_inset

 and will extract the features of each individual image before comparison
 between them.
 However, many of these papers that propose such a network only show results
 when it is trained and tested on a specific dataset of still images.
 They do not integrate it with a tracking system to see how it can perform
 in the real world, as we propose to do.
 Also, this previous work has all been done on colour imagery.
 Very little work has been done to try and solve the re-identification problem
 in thermal imagery, save for our own previous work 
\begin_inset CommandInset citation
LatexCommand cite
key "Robson2017"

\end_inset

.
 Therefore, the major research aim of our project, and where we are advancing
 the state of the art, is to see if such a network architecture can successfully
 be applied to thermal imagery, and whether it can perform effectively as
 a part of a full re-identification system in real time.
 
\end_layout

\begin_layout Section
Solution
\end_layout

\begin_layout Standard
Having established the problem that we want to solve, we will now break
 down the most important elements of our solution, giving an overview of
 the structure of our implementation and a description of the elements used.
\end_layout

\begin_layout Subsection
Implementation Structure and Tools Used 
\end_layout

\begin_layout Standard
The structure of our implementation is shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Logic-of-our"

\end_inset

.The implementation will begin by opening each video file, or live camera
 feed, and concurrently running the real time target detection code on these.
 Each time this code identifies a person, it checks whether it has a TLD
 tracker object associated with them or not.
 This is decision point [A] in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Logic-of-our"

\end_inset

.
 If not, a new TLD tracker object is created and this person is compared
 to the other people that have been seen previously using the siamese CNN,
 and if they are deemed to be sufficiently similar to one of these people,
 then they are re-identified as the same person, else the system creates
 a new person object.
 This is decision point [B] in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Logic-of-our"

\end_inset

.
 Each of these person objects has an associated TLD tracker and set of previous
 observations, and these are used to facilitate the comparison between targets,
 and are updated each time the target is successfully identified.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement b
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename images/Project Activity Diagram Fourth Year.png
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Logic-of-our"

\end_inset

Logic of our Re-Identification System
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Many of the computer vision techniques that we use are complex to implement.
 Therefore, the OpenCV library 
\begin_inset CommandInset citation
LatexCommand cite
key "opencv_library"

\end_inset

 has been used to allow us to use stable, well tested code.
 However, the implementation of TLD code in OpenCV is suboptimal, so we
 have used the multi-object implementation of TLD from 
\begin_inset CommandInset citation
LatexCommand cite
key "Lutz2015"

\end_inset

.
 The neural network side of the project is written in Keras 
\begin_inset CommandInset citation
LatexCommand cite
key "chollet2015keras"

\end_inset

, using the Tensorflow backend.
 
\end_layout

\begin_layout Subsection
Real Time Person Detection
\end_layout

\begin_layout Standard
Before we can start concerning ourselves with deep learning based person
 re-identification, we must be able to identify whether a person is present
 in the image at all.
 This will give us a region of interest to pass to the neural network for
 re-identification, and so we can use TLD to track targets through single
 camera views.
 
\end_layout

\begin_layout Subsubsection
Background Subtraction
\end_layout

\begin_layout Standard
The first stage of this process is background subtraction from the static
 camera viewpoint.
 We use the Mixture of Gaussians (MoG) technique to facilitate this, taking
 inspiration from 
\begin_inset CommandInset citation
LatexCommand cite
key "Zivkovic2004,Zivkovic"

\end_inset

.
 This technique works by building up a background model over multiple camera
 frames, modelling each of the pixels using multiple Gaussians.
 Using a Gaussian over the last 
\begin_inset Formula $N$
\end_inset

 frames, where 
\begin_inset Formula $N$
\end_inset

 is given by a parameter specifying the rate at which the background model
 is updated, is far more memory efficient than storing all the pixel values
 across the entire video capture.
 This update rate is determined by the trade off between being fast enough
 to absorb objects that have become stationary into the background and being
 slow enough to allow the detection of slow moving objects.
\end_layout

\begin_layout Standard
As the program runs through the video, during each new frame, a Gaussian
 for each pixel is evaluated using a simple heuristic to determine which
 is most likely to correspond to the background model.
 Pixels that do not match closely enough with these background Gaussians
 are classified as foreground elements and added to a new image in the code.
 Once these foreground pixels are identified and built into a foreground
 mask, erosion and dilation image operators 
\begin_inset CommandInset citation
LatexCommand cite
key "solomonbreckon10fundamentals"

\end_inset

 are used to clean up these results.
 From here, we use the contour detection to find the connected components
 and draw bounding boxes around these contours.
 
\end_layout

\begin_layout Subsubsection
Person Identification
\end_layout

\begin_layout Standard
Having identified the foreground objects, we must now determine whether
 they are people.
 The implementation uses Histogram of Oriented Gradients (HOG), discussed
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Dalal"

\end_inset

.
 This method works by performing edge gradient calculation on the bounding
 box identified by the background subtractor.
 From here cell histograms are computed, with each of the histogram entries
 filled by gradient magnitudes.
 These histograms are then used to create overlapping block histograms of
 the adjacent cells.
 These block histograms are then concatenated to give a HOG descriptor,
 a high dimensional vector.
 This HOG descriptor is then passed to a pre-trained machine learning algorithm,
 in this case a Support Vector Machine (SVM).
 If this comes up with a positive identification, then it is classified
 as a person and will be given an associated TLD tracker object.
 In our previous work in 
\begin_inset CommandInset citation
LatexCommand cite
key "Robson2017"

\end_inset

, we had a tradeoff between HOG and Haar cascades due to HOG being slower.
 However, as we are using a TLD tracker this year, there will be far fewer
 calls to the person detector so we can use the superior performance of
 HOG without fear of the slower runtime making a large impact on the overall
 performance.
 
\end_layout

\begin_layout Subsubsection
Person Tracking using Track-Learn-Detect 
\end_layout

\begin_layout Standard
Once we know where the people are in the camera view, we can track their
 movement.
 We use the Track-Learn-Detect algorithm, originally proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "Kalal2010"

\end_inset

.
 This algorithm was originally intended to facilitate the long term tracking
 of an unknown object in a video stream.
 The algorithm works by breaking down the person tracking task into tracking,
 learning and detection.
 The tracker follows the object between frames.
 The detector corrects the tracker if necessary based on previous observations.
 The learning estimates detector’s errors and updates it to avoid these
 errors in the future.
 The relationship between these components is shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Block-diagram-of"

\end_inset

.
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/TLDDiagram.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Block diagram of the TLD framework, from 
\begin_inset CommandInset citation
LatexCommand cite
key "Kalal2010"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Block-diagram-of"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In our early experimentation, we found that the learning part of this algorithm
 meant that it was attempting to perform re-identification when people left
 and re-entered a video stream, or appeared in a different one.
 Therefore, we had to cut some of the functionality out of TLD for this
 implementation, so that each object was deleted when it left the scene
 or became obstructed.
 This eliminated the miss-classifications that the limited re-identification
 capabilities of this algorithm were causing, enabling us to effectively
 track people across the scene, whilst using our neural network for the
 re-identification.
 
\end_layout

\begin_layout Standard
Consequently, in our this implementation each person currently present in
 any of the camera views has an associated TLD tracker object, which is
 created on the first HOG identification in this neighbourhood, and classified
 by the neural network.
 The tracker then follows this person through the frame until either the
 person leaves the frame or HOG is no longer getting a positive identification
 within the tracker.
 If either of these conditions occurs, the TLD object is deleted.
 In the case of no HOG identification being present, this indicated that
 the tracker has incorrectly predicted the position or rate of movement
 of the person and has lost them.
 A new tracker will then be created to replace it in the next frame when
 HOG detects this person again.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:An-example-of"

\end_inset

 shows the TLD system in action following a human target through a frame,
 as well as displaying the positive and negative patches used to inform
 the learning part of TLD.
 The dotted boxes represent other possible areas that the TLD system thinks
 the target may be in, while the blue tracking points show the features
 on the person that the system is looking for in each frame.
 Some are not found, so the region containing the best of these is chosen.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/TLD.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example of TLD tracking a person through a scene.
 The images on the left are negative patches, and the images on the right
 are positive patches
\begin_inset CommandInset label
LatexCommand label
name "fig:An-example-of"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Deep Learning
\end_layout

\begin_layout Standard
We can now move on to our approach to deep learning based re-identification,
 considering our dataset and our architecture choices.
 
\end_layout

\begin_layout Subsubsection
Training and Validation Dataset
\end_layout

\begin_layout Standard
The model was trained on the dataset collected for our previous work 
\begin_inset CommandInset citation
LatexCommand cite
key "Robson2017"

\end_inset

, as this contained video of multiple people from different views.
 The size of these images was fixed at 258x128, to ensure that the size
 of the file had no effect on the training.
 These images were then formed into a balanced dataset of positive and negative
 pairs.
 The positive pairs contain two images of the same person, labelled 1, and
 negative pairs contain different people, labelled 0.
 We then applied some data augmentation to the images, following the advice
 of 
\begin_inset CommandInset citation
LatexCommand cite
key "Almazan2018"

\end_inset

, with the aim of introducing more variation into our dataset and enable
 it to cope better with difficult real world situations.
 This data was then split 80:20 into a train and validation set.
\end_layout

\begin_layout Subsubsection
Network Architecture
\end_layout

\begin_layout Standard
Taking inspiration from the state of the art literature in the area of open
 world person re-identification, our network architecture is a Deep Siamese
 CNN.
 This means that the network is trained on pairs of images, and attempts
 to determine whether these images show the same person or a different person.
 The two networks which are each fed an image have exactly the same architecture
 and weights, as they are trained together.
 Our CNN consists of convolutional layers, pooling layers and fully connected
 layers.
 
\end_layout

\begin_layout Standard
The convolutional layers operate directly on the image to reduce the complexity
 of the input in a manner that is both meaningful and structured.
 This enables the network to have fewer neurons than a traditional fully
 connected feedforward network would have if operating on an image.
 This enables the convolutional neural network to be far deeper with fewer
 parameters.
 This resolves the vanishing or exploding gradients problem that would otherwise
 occur, as a greater number of training parameters in the early layers of
 the network would make it unstable, and prone to the weights becoming much
 too large (exploding gradients) or much too small (vanishing gradients).
 
\end_layout

\begin_layout Standard
Pooling layers then down-sample their input, looking at a neighbourhood
 of pixels and, in the case of max pooling that we have used here, output
 the maximum.
 The aim of this is to provide robustness to the changes in the spatial
 location of features across the dataset and to reduce input dimensionality.
 This is also an effective tool to control overfitting, as it reduces the
 number of training parameters and ensures that it is the general regions
 where features are present that are learned, not the specific pixels of
 the images in the training set in which they are present.
 
\end_layout

\begin_layout Standard
Finally, we flatten the output of our last pooling layer to a single dimension
 and then pass it to a fully connected layer.
 The high dimensional vector output of this layer will be the representation
 of this image, and we will use the euclidean distances between these vectors
 to determine whether the inputs show the same person or different people.
 
\end_layout

\begin_layout Standard
At various stages of the network, we have some dropout layers.
 The purpose of dropout is to eliminate a proportion of the training data
 at each epoch to help to prevent overfitting, as the network is being trained
 on a different dataset each epoch.
 The goal is therefore to force the network to learn more robust features
 that are useful in conjunction with random subsets of the other neurons
 so that good performance is still achieved when some neurons are removed.
\end_layout

\begin_layout Standard
The loss function to be used for this network was proposed by the authors
 of 
\begin_inset CommandInset citation
LatexCommand cite
key "Hadsell2006"

\end_inset

, and is given by Equation (1) below.
 This paper defines a contrastive loss function, which maps high dimensional
 inputs to lower dimensional outputs, given distances between samples in
 its input space.
 These distances between samples are supplied by the Euclidean distance.
 The formula for this function is given by Equation (1) below, where 
\begin_inset Formula $Y$
\end_inset

 is the label (0 or 1), 
\begin_inset Formula $m$
\end_inset

 is the margin that determines the the maximum euclidean distance between
 two points that will influence the loss function.
 The purpose of this is to ensure that vastly different images do not have
 a disproportional effect on the learning outcome.
 
\begin_inset Formula $D$
\end_inset

 is the Euclidean distance, which is defined here by equation (2) below,
 where 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 are the input images and 
\begin_inset Formula $F$
\end_inset

 represents the feature vectors output by the siamese pair of identical
 CNNs.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
L=(1-Y)\frac{1}{2}(D)^{2}+Y\frac{1}{2}\{max(0,m-D)\}^{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
D=\sqrt{\{F(X_{1})-F(X_{2})\}^{2}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The architecture that we chose to use is made up of two convolutional layers,
 a max pooling layer, a dropout layer, two convolutional layers, a max pooling
 layer, a dropout layer, a flatten layer and a dense fully connected layer.
 The parameters for these layers were all determined by an exhaustive grid
 search using the Hyperas library 
\begin_inset CommandInset citation
LatexCommand cite
key "Pumperla2017"

\end_inset

.
 For the convolutional layers the parameters we were experimenting with
 were the dimensionality of the output space (i.e.
 the number of output filters in the convolution) and the kernel size, specifyin
g the width and height of the 2D convolution window.
 For the poolling layers we experimented with the pool size, which gave
 the factors to downscale the input by.
 For the dropout layers we experimented with how much of the dataset should
 be removed at each point, and for the dense layer we experimented with
 the dimensionality of the input space.
 The chosen parameters are shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:The-architecture-of"

\end_inset

.
 We also experimented with various activation functions for the convolutional
 layers and the dense layers, before settling on the rectified linear unit
 (ReLU) activation function.
 This was chosen as we do not require negative values for these layers,
 and this activation function offers quick training as well as quick adjustments
 to the positive weights.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
explain ReLU better
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/architecture.jpg
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
The architecture of our CNN
\begin_inset CommandInset label
LatexCommand label
name "fig:The-architecture-of"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Need to ensure the parameters chosen as justifiable and explained, without
 simply saying 
\begin_inset Quotes eld
\end_inset

grid search said so
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
We now present the performance that our system has been able to acheive,
 both as a standalone CNN and as part of a full re-identification system.
\end_layout

\begin_layout Subsection
The Evaluation Dataset
\end_layout

\begin_layout Standard
The data used to evaluate the re-identification system was gathered with
 three cameras at Durham University.
 The cameras and their fields of view are arranged as they are in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Setup-of-Cameras"

\end_inset

(a), with the cameras labelled as camera 
\begin_inset Formula $\alpha$
\end_inset

, 
\begin_inset Formula $\beta$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

 respectively, and their fields of view shown by the matching coloured lines.
 The images seen by the cameras are shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Setup-of-Cameras"

\end_inset

(b).
 The dataset contains five people.
 These videos make up the data we will use to test the full re-identification
 system, and the person images extracted from them make up the evaluation
 dataset for the CNN.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/dataset.png
	scale 15

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/datasetEmpty.png
	scale 13

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Setup-of-Cameras"

\end_inset

Position (a) and field of view (b) of the cameras used to record our data
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
The Deep Learning System
\end_layout

\begin_layout Standard
When designing our network we did many training runs to determine the best
 architecture, and then performed extensive grid searches to determine the
 best parameters.
 While training the network, we were mainly concerned about the loss, from
 the contrastive loss function defined in Section 3, and the accuracy.
 We calculate the accuracy based on a re-identification threshold of 0.5.
 This means that a pair of images with a Euclidean distance of less than
 0.5 would be classified as the same person, whereas a pair with a Euclidean
 distance of greater than 0.5 would be classified as a different person.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Loss-and-Accuracy"

\end_inset

 shows how the loss and accuracy varied on our training and validation sets
 as the network was trained over 100 epochs.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/train_loss.png
	scale 15

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Loss on the train dataset
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/train_acc.png
	scale 15

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Accuracy on the train dataset
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/test_loss.png
	scale 15

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Loss on the validation dataset
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/test_acc.png
	scale 15

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Accuracy on the validation dataset
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Loss and Accuracy results during training of the network over 100 epochs
\begin_inset CommandInset label
LatexCommand label
name "fig:Loss-and-Accuracy"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Confusion-Matrices"

\end_inset

 shows the confusion matrices of the network when predicting all three datasets.
 These compare the output of the network against our ground truth labels
 and report the number of true positives (TP), where a pair is correctly
 classified as the same person, false positives (FP), where a pair is wrongly
 classified as the same person, false negatives (FN), where a pair is wrongly
 classified as different people and true negatives (TN), where a pair is
 correctly classified as different people.
 Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Classification-Reports"

\end_inset

 shows the classification reports for these datasets, reporting the accuracy,
 which is the percentage of the dataset that was correctly classified, precision
, which is the percentage of the pairs that we labeled the same person actually
 were the same person, recall, which is the percentage of the pairs that
 were the same person that we correctly labelled and F1-Score, which is
 the average of precision and recall, taking into account both false positives
 and false negatives.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Predicted Label
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Negative
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ground Truth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
TP = 5809
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FN = 2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Negative
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FP = 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
TN = 5809
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Confusion matrix for training set
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Predicted Label
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Negative
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ground Truth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
TP = 1229
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FN = 52
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Negative
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FP = 26
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
TN = 1255
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Confusion matrix for validation set
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Predicted Label
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Negative
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Ground Truth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Positive
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
TP = 3807
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FN = 138
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Negative
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FP = 194
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
TN = 3751
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Confusion matrix for evaluation set
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Confusion Matrices
\begin_inset CommandInset label
LatexCommand label
name "tab:Confusion-Matrices"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="5">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Metric
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Precision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recall
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F1 Score
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Value
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
99.97%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
99.97%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
99.97%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
99.97%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Classification report for training set
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="5">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Metric
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Precision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recall
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F1 Score
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Value
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
96.96%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
97.93%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
95.94%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
96.92%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Classification report for validation set
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="5">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Metric
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Precision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recall
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F1 Score
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Value
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
95.79%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
96.50%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
95.15%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
95.82%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Classification report for evaluation set
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Classification Reports
\begin_inset CommandInset label
LatexCommand label
name "tab:Classification-Reports"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Results-of-the"

\end_inset

 is a graphical representation of the Euclidean distances reported by the
 network on the train, validation and evaluation dataset.
 These graphs show a clear split between the pairs of images of the same
 person and pairs of images of different people.
 The split for the training set is more clear than the split for the validation
 and evaluation set.
 This improved performance on the training set is expected, as these are
 the exact images the network was trained on so some level of overfitting
 was expected.
 The fact that the validation set and evaluation set distribution is similar
 shows that this overfitting was to the actual training images, rather than
 the dataset they were taken from, as the also includes the validation dataset.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/trainResults.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/validationResults.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/evalResults.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Results of the CNN on the Train, Validation and Evaluation Dataset
\begin_inset CommandInset label
LatexCommand label
name "fig:Results-of-the"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Re-Identification Performance
\end_layout

\begin_layout Section
Evaluation
\end_layout

\begin_layout Standard
We will now evaluate the strengths, weaknesses and limitations of the research
 that has been presented here.
 
\end_layout

\begin_layout Subsection
Real Time Person Detection System
\end_layout

\begin_layout Standard
The inclusion of the TLD tracker has greatly improved the performance of
 our person tracking system in comparison to our previous work in 
\begin_inset CommandInset citation
LatexCommand cite
key "Robson2017"

\end_inset

.
 We are getting far fewer misclassifications from HOG as it is being called
 far fewer times due to the TLD tracker.
 The tracker performs particularly well on cameras 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

, but less well on camera 
\begin_inset Formula $\gamma$
\end_inset

.
 This seems to be due to the fact that each person is moving directly across
 the field of view of camera 
\begin_inset Formula $\gamma$
\end_inset

, which the tracker seems to be less able to deal with than cameras 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

, where the people are moving away from or towards the camera as well as
 across its field of view.
 Through all of our preliminary testing on various datasets, the trend that
 we observed was that directly horizontal movement caused problems for the
 tracker.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
SUGGEST WHY
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
IMAGES OF DIFFERENT MOVEMENT DIRECTIONS
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Deep Learning System
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Loss-and-Accuracy"

\end_inset

 shows use that the training and validation patterns of the network matched
 very closely, with the loss steadily decreasing and the accuracy steadily
 increasing before both flattened out towards the end of training.
 The use of 100 epochs gave both the loss and the accuracy time to stabilise,
 and ensure that the results we got for these, and the trained network as
 a whole were the best that could be attained.
 
\end_layout

\begin_layout Standard
The results from Tables 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Confusion-Matrices"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Classification-Reports"

\end_inset

 are encouraging, as all are high percentages across all datasets, giving
 us confidence that our system is working well and has learnt useful features
 that work on multiple datasets.
 The fact that precision and recall are close together means that the numbers
 of false positives and false negatives are fairly close, as shown by Table
 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Confusion-Matrices"

\end_inset

.
 As missclassifying different people as the same person and missclassifying
 the same person as different people are both reasonably destructive to
 our full re-identification system, the fact that all of these metrics get
 good results means that we have not sacrificed proficiency in one of these
 cases for the other.
 
\end_layout

\begin_layout Standard
The graphs from Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Results-of-the"

\end_inset

 show that there is some overfitting to the training set, but given that
 the validation set is based on the same videos as the training set, 
\end_layout

\begin_layout Subsection
Re-Identification System
\end_layout

\begin_layout Subsection
Appraisal of Project Organisation
\end_layout

\begin_layout Standard
In the implementation of this system we have used a modular approach, implementi
ng each major component separately.
 The multi-object implementation of TLD from 
\begin_inset CommandInset citation
LatexCommand cite
key "Lutz2015"

\end_inset

 was originally written in C as a header-file based library, which was incompati
ble with the rest of our C++ codebase.
 We therefore had to spend considerable time rewriting this library so it
 was compatible with our system, as well as refactoring and adding new functions
 to suppress the erroneous attempts at re-identification and to enable us
 to delete TLD objects from the system.
 Also, as Keras uses Tensorflow as its backend, and OpenCV has the ability
 to read and use Tensorflow networks.
 However, this turned out to be incompatible, which required us to spend
 much time trying to import it correctly, before deciding that the superior
 method was to use both C++ and python for the separate components and connect
 them at system level.
 Despite this issue, and our aims were accomplished, enabling us to contribute
 to the state of the art.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "My Collection"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_body
\end_document
